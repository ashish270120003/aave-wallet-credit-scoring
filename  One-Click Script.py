# -*- coding: utf-8 -*-
"""Untitled50.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uP68vgRaqUZ9BZ0cXpdploks9ox6PtB_
"""

# prompt: /content/user-wallet-transactions.json.zip

!unzip /content/user-wallet-transactions.json.zip

# prompt: /content/user-wallet-transactions.json.zip

import json

# Define the path to the unzipped JSON file
file_path = '/content/user-wallet-transactions.json' # Assuming the zip contained this file name

# Open and read the JSON file
with open(file_path, 'r') as f:
    data = json.load(f)

# Now 'data' contains the content of the JSON file as a Python dictionary or list
# You can print it or process it further as needed
# print(data)

# prompt: print(df.head())
# print(df.columns)

import pandas as pd

# Assuming 'data' is a list of dictionaries (common for JSON)
# Create a DataFrame from the loaded JSON data
df = pd.DataFrame(data)

# Now you can print the head and columns of the DataFrame
print(df.head())
df.columns

# prompt: Feature Engineering
# Goal: Extract wallet-level behavior metrics.
# Group by wallet address and compute:
# num_deposits
# total_deposit_amount
# num_borrows
# borrow_to_deposit_ratio
# total_repay_amount / total_borrow_amount
# num_liquidation_calls
# active_days, first_tx, last_tx, tx_frequency
# Create feature_engineering.py

# feature_engineering.py
import pandas as pd

def feature_engineer_wallet_behavior(df):
    """
    Extracts wallet-level behavior metrics from transaction data.

    Args:
        df (pd.DataFrame): DataFrame containing transaction data with columns
                           like 'userWallet', 'action', 'actionData',
                           'timestamp', etc.

    Returns:
        pd.DataFrame: DataFrame with wallet-level features.
    """
    # Ensure timestamp column is in datetime format
    df['timestamp'] = pd.to_datetime(df['timestamp'])

    # Group by wallet address
    grouped = df.groupby('userWallet')

    # Compute various metrics
    wallet_features = grouped.agg(
        num_deposits=('action', lambda x: (x == 'deposit').sum()),
        # total_deposit_amount=('amount', lambda x: x[df['action'] == 'deposit'].sum()), # This needs to extract amount from actionData
        num_borrows=('action', lambda x: (x == 'borrow').sum()),
        # total_borrow_amount=('amount', lambda x: x[df['action'] == 'borrow'].sum()), # This needs to extract amount from actionData
        # total_repay_amount=('amount', lambda x: x[df['action'] == 'repay'].sum()), # This needs to extract amount from actionData
        num_liquidation_calls=('action', lambda x: (x == 'liquidation').sum()),
        first_tx=('timestamp', 'min'),
        last_tx=('timestamp', 'max'),
        num_transactions=('action', 'count')
    ).reset_index()

    # Calculate derived metrics
    # wallet_features['borrow_to_deposit_ratio'] = wallet_features['total_borrow_amount'] / wallet_features['total_deposit_amount'].replace(0, pd.NA)
    # wallet_features['repay_to_borrow_ratio'] = wallet_features['total_repay_amount'] / wallet_features['total_borrow_amount'].replace(0, pd.NA)

    # Calculate active days and transaction frequency
    wallet_features['active_days'] = (wallet_features['last_tx'] - wallet_features['first_tx']).dt.days + 1 # Add 1 to include both start and end day
    # Avoid division by zero for wallets with only one transaction
    wallet_features['tx_frequency'] = wallet_features['num_transactions'] / wallet_features['active_days'].replace(0, 1)


    return wallet_features

# Assuming 'df' is already loaded from the previous steps
# Call the feature engineering function
wallet_behavior_df = feature_engineer_wallet_behavior(df)

# Display the resulting features
print(wallet_behavior_df.head())

# prompt: Create feature_engineering.py to generate a CSV:
# bash
# Copy
# Edit

import json
import pandas as pd

# Define the path to the unzipped JSON file
file_path = '/content/user-wallet-transactions.json' # Assuming the zip contained this file name

# Open and read the JSON file
with open(file_path, 'r') as f:
    data = json.load(f)

# Create a DataFrame from the loaded JSON data
df = pd.DataFrame(data)

# Now you can print the head and columns of the DataFrame
print(df.head())
print(df.columns)


def feature_engineer_wallet_behavior(df):
    """
    Extracts wallet-level behavior metrics from transaction data.

    Args:
        df (pd.DataFrame): DataFrame containing transaction data with columns
                           like 'userWallet', 'action', 'actionData',
                           'timestamp', etc.

    Returns:
        pd.DataFrame: DataFrame with wallet-level features.
    """
    # Ensure timestamp column is in datetime format
    df['timestamp'] = pd.to_datetime(df['timestamp'])

    # Group by wallet address
    grouped = df.groupby('userWallet')

    # Compute various metrics
    wallet_features = grouped.agg(
        num_deposits=('action', lambda x: (x == 'deposit').sum()),
        num_borrows=('action', lambda x: (x == 'borrow').sum()),
        num_liquidation_calls=('action', lambda x: (x == 'liquidation').sum()),
        first_tx=('timestamp', 'min'),
        last_tx=('timestamp', 'max'),
        num_transactions=('action', 'count')
    ).reset_index()

    # Calculate derived metrics
    # Calculate active days and transaction frequency
    wallet_features['active_days'] = (wallet_features['last_tx'] - wallet_features['first_tx']).dt.days + 1 # Add 1 to include both start and end day
    # Avoid division by zero for wallets with only one transaction
    wallet_features['tx_frequency'] = wallet_features['num_transactions'] / wallet_features['active_days'].replace(0, 1)

    return wallet_features

# Assuming 'df' is already loaded from the previous steps
# Call the feature engineering function
wallet_behavior_df = feature_engineer_wallet_behavior(df)

# Display the resulting features
print(wallet_behavior_df.head())

# Save the resulting DataFrame to a CSV file
wallet_behavior_df.to_csv('wallet_features.csv', index=False)

print("\nWallet features saved to wallet_features.csv")

# prompt: Scoring Logic
# Goal: Map behavior to a 0–1000 credit score.
# Implement scoring_model.py:
# Normalize all features (MinMaxScaler)
# Assign weights based on intuition or basic analytics
# Scale final score to 0–1000 using:
# python
# Copy
# Edit
# Calculate score using a weighted formula:

import pandas as pd
# scoring_model.py
from sklearn.preprocessing import MinMaxScaler
import numpy as np

def score_wallet_behavior(df):
    """
    Scores wallet behavior features and scales the final score to 0-1000.

    Args:
        df (pd.DataFrame): DataFrame containing wallet-level features.

    Returns:
        pd.DataFrame: DataFrame with added 'credit_score' column.
    """
    # Select numerical features for scoring (exclude identifiers and dates)
    scoring_features = [
        'num_deposits',
        'num_borrows',
        'num_liquidation_calls',
        'num_transactions',
        'active_days',
        'tx_frequency'
    ]

    # Ensure selected features exist in the DataFrame
    valid_scoring_features = [f for f in scoring_features if f in df.columns]
    if not valid_scoring_features:
        print("Warning: No valid scoring features found in the DataFrame.")
        df['credit_score'] = 0
        return df

    features_df = df[valid_scoring_features].copy()

    # Normalize features using MinMaxScaler
    scaler = MinMaxScaler()
    normalized_features = scaler.fit_transform(features_df)
    normalized_df = pd.DataFrame(normalized_features, columns=[f'{col}_normalized' for col in valid_scoring_features], index=df.index)

    # Combine original df with normalized features
    df = pd.concat([df, normalized_df], axis=1)

    # Assign weights based on intuition (can be adjusted)
    # Higher weight for positive behaviors (deposits, transactions) and lower for negative (liquidations)
    weights = {
        'num_deposits_normalized': 0.2,
        'num_borrows_normalized': 0.1,
        'num_liquidation_calls_normalized': -0.3, # Penalize liquidations
        'num_transactions_normalized': 0.2,
        'active_days_normalized': 0.1,
        'tx_frequency_normalized': 0.1 # Frequency is often good
    }

    # Calculate a preliminary score using weighted sum
    df['preliminary_score'] = 0
    for feature, weight in weights.items():
        if feature in df.columns:
            df['preliminary_score'] += df[feature] * weight

    # Scale the final score to 0-1000
    # We need to scale the 'preliminary_score' which can be negative due to negative weights
    # First, scale to 0-1 range based on its min and max values
    min_prelim = df['preliminary_score'].min()
    max_prelim = df['preliminary_score'].max()

    # Handle the case where min_prelim == max_prelim (e.g., all scores are the same)
    if max_prelim == min_prelim:
        df['credit_score'] = 500 # Assign a neutral score if no variation
    else:
        df['credit_score'] = (df['preliminary_score'] - min_prelim) / (max_prelim - min_prelim) * 1000

    # Ensure score is an integer
    df['credit_score'] = df['credit_score'].astype(int)

    # Drop the preliminary score and normalized columns if desired
    # df = df.drop(columns=['preliminary_score'] + [f'{col}_normalized' for col in valid_scoring_features])


    return df

# Assuming 'wallet_behavior_df' is the output from feature engineering
# Call the scoring function
scored_wallet_df = score_wallet_behavior(wallet_behavior_df.copy()) # Pass a copy to avoid modifying original feature df

# Display the resulting DataFrame with scores
print("\nScored Wallet Behavior:")
print(scored_wallet_df[['userWallet', 'credit_score']].head())

# You can save the scored DataFrame to a new CSV if needed
scored_wallet_df.to_csv('scored_wallet_features.csv', index=False)
print("\nScored wallet features saved to scored_wallet_features.csv")

# prompt: One-Click Script
# Goal: Combine everything into one script.
# credit_score_generator.py should:
# Load JSON
# Generate features
# Calculate scores
# Save as CSV
# bash
# Copy
# Edit

import json
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
import numpy as np
import os

# Define the zip file path
zip_file_path = '/content/user-wallet-transactions.json.zip'
# Define the expected unzipped JSON file path
json_file_path = '/content/user-wallet-transactions.json'

# Unzip the file if it exists
if os.path.exists(zip_file_path):
    print(f"Unzipping {zip_file_path}...")
    !unzip -o {zip_file_path} -d /content/ # Use -o to overwrite if needed
else:
    print(f"Zip file not found at {zip_file_path}. Skipping unzip.")


# credit_score_generator.py

def load_data(file_path):
    """
    Loads JSON data from a file into a Pandas DataFrame.
    """
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"Data file not found at {file_path}")

    with open(file_path, 'r') as f:
        data = json.load(f)

    # Assuming data is a list of dictionaries
    df = pd.DataFrame(data)
    print(f"Loaded {len(df)} transactions.")
    return df

def feature_engineer_wallet_behavior(df):
    """
    Extracts wallet-level behavior metrics from transaction data.
    """
    print("Starting feature engineering...")
    # Ensure timestamp column is in datetime format
    df['timestamp'] = pd.to_datetime(df['timestamp'])

    # Group by wallet address
    grouped = df.groupby('userWallet')

    # Compute various metrics
    wallet_features = grouped.agg(
        num_deposits=('action', lambda x: (x == 'deposit').sum()),
        num_borrows=('action', lambda x: (x == 'borrow').sum()),
        num_liquidation_calls=('action', lambda x: (x == 'liquidation').sum()),
        first_tx=('timestamp', 'min'),
        last_tx=('timestamp', 'max'),
        num_transactions=('action', 'count')
    ).reset_index()

    # Calculate derived metrics
    # Calculate active days and transaction frequency
    wallet_features['active_days'] = (wallet_features['last_tx'] - wallet_features['first_tx']).dt.days + 1 # Add 1 to include both start and end day
    # Avoid division by zero for wallets with only one transaction
    wallet_features['tx_frequency'] = wallet_features['num_transactions'] / wallet_features['active_days'].replace(0, 1)

    print(f"Generated features for {len(wallet_features)} wallets.")
    return wallet_features

def score_wallet_behavior(df):
    """
    Scores wallet behavior features and scales the final score to 0-1000.
    """
    print("Starting scoring...")
    # Select numerical features for scoring (exclude identifiers and dates)
    scoring_features = [
        'num_deposits',
        'num_borrows',
        'num_liquidation_calls',
        'num_transactions',
        'active_days',
        'tx_frequency'
    ]

    # Ensure selected features exist in the DataFrame
    valid_scoring_features = [f for f in scoring_features if f in df.columns]
    if not valid_scoring_features:
        print("Warning: No valid scoring features found in the DataFrame. Assigning score 0.")
        df['credit_score'] = 0
        return df

    features_df = df[valid_scoring_features].copy()

    # Normalize features using MinMaxScaler
    scaler = MinMaxScaler()
    # Handle potential NaNs or infinite values before scaling
    features_df = features_df.replace([np.inf, -np.inf], np.nan).fillna(features_df.median()) # Or use mean(), 0, etc. based on data

    normalized_features = scaler.fit_transform(features_df)
    normalized_df = pd.DataFrame(normalized_features, columns=[f'{col}_normalized' for col in valid_scoring_features], index=df.index)

    # Combine original df with normalized features
    df = pd.concat([df.reset_index(drop=True), normalized_df.reset_index(drop=True)], axis=1)

    # Assign weights based on intuition (can be adjusted)
    weights = {
        'num_deposits_normalized': 0.2,
        'num_borrows_normalized': 0.1,
        'num_liquidation_calls_normalized': -0.3, # Penalize liquidations
        'num_transactions_normalized': 0.2,
        'active_days_normalized': 0.1,
        'tx_frequency_normalized': 0.1
    }

    # Calculate a preliminary score using weighted sum
    df['preliminary_score'] = 0
    for feature, weight in weights.items():
        if feature in df.columns:
            df['preliminary_score'] += df[feature] * weight

    # Scale the final score to 0-1000
    min_prelim = df['preliminary_score'].min()
    max_prelim = df['preliminary_score'].max()

    if max_prelim == min_prelim:
        df['credit_score'] = 500 # Assign a neutral score if no variation
    else:
        df['credit_score'] = (df['preliminary_score'] - min_prelim) / (max_prelim - min_prelim) * 1000

    # Ensure score is an integer
    df['credit_score'] = df['credit_score'].astype(int)

    # Optional: Drop intermediate columns
    cols_to_drop = ['preliminary_score'] + [f'{col}_normalized' for col in valid_scoring_features]
    df = df.drop(columns=[col for col in cols_to_drop if col in df.columns])


    print("Scoring complete.")
    return df[['userWallet', 'credit_score']] # Return only wallet and score

# --- Main Execution ---

# 1. Load Data
try:
    transaction_df = load_data(json_file_path)
except FileNotFoundError as e:
    print(e)
    # Handle the error, perhaps exit or create a dummy empty dataframe
    transaction_df = pd.DataFrame() # Create empty df to avoid errors later
    print("Proceeding with an empty DataFrame.")


if not transaction_df.empty:
    # 2. Generate Features
    wallet_features_df = feature_engineer_wallet_behavior(transaction_df)

    # 3. Calculate Scores
    scored_wallets_df = score_wallet_behavior(wallet_features_df.copy()) # Pass a copy

    # 4. Save Results
    output_csv_path = 'wallet_credit_scores.csv'
    scored_wallets_df.to_csv(output_csv_path, index=False)

    print(f"\nCredit scores saved to {output_csv_path}")
    print("\nSample Scores:")
    print(scored_wallets_df.head())

else:
    print("\nCannot proceed with feature engineering and scoring due to empty or missing data.")
    # Optionally create an empty output file
    pd.DataFrame(columns=['userWallet', 'credit_score']).to_csv('wallet_credit_scores.csv', index=False)
    print("Created an empty 'wallet_credit_scores.csv' file.")

# prompt: Analysis & Visualization
# Goal: Understand wallet behavior across score ranges.
# Create analysis.md with:
# Score distribution bar chart (0–100, 100–200, ..., 900–1000)
# Compare high vs low score wallets (e.g., avg deposit, liquidation rate)
# Highlight risk patterns (bots, exploiters, good actors)
# Use matplotlib to create plots:
# python
# Copy
# Edit

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd

# Load the scored data
try:
    scored_wallets_df = pd.read_csv('wallet_credit_scores.csv')
    print("Loaded scored_wallet_features.csv for analysis.")
    print(scored_wallets_df.head())
except FileNotFoundError:
    print("Error: 'wallet_credit_scores.csv' not found. Please run the previous steps to generate it.")
    scored_wallets_df = pd.DataFrame() # Create empty df to avoid errors later

if not scored_wallets_df.empty:

    # --- Analysis & Visualization ---

    # 1. Score distribution bar chart (0–100, 100–200, ..., 900–1000)
    print("\nGenerating Score Distribution Chart...")
    score_bins = range(0, 1001, 100)
    # Create a categorical column for bins
    scored_wallets_df['score_bin'] = pd.cut(scored_wallets_df['credit_score'], bins=score_bins, right=False, labels=[f'{i}-{i+99}' for i in range(0, 1000, 100)])

    # Count wallets per bin
    score_distribution = scored_wallets_df['score_bin'].value_counts().sort_index()

    plt.figure(figsize=(12, 6))
    score_distribution.plot(kind='bar')
    plt.title('Distribution of Wallet Credit Scores')
    plt.xlabel('Credit Score Range')
    plt.ylabel('Number of Wallets')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.show()


    # 2. Compare high vs low score wallets
    print("\nComparing High vs Low Score Wallets...")

    # Define high and low score thresholds (can be adjusted)
    low_score_threshold = scored_wallets_df['credit_score'].quantile(0.25) # Bottom 25%
    high_score_threshold = scored_wallets_df['credit_score'].quantile(0.75) # Top 25%

    low_score_wallets = scored_wallets_df[scored_wallets_df['credit_score'] <= low_score_threshold]
    high_score_wallets = scored_wallets_df[scored_wallets_df['credit_score'] >= high_score_threshold]

    print(f"Number of low score wallets (score <= {low_score_threshold:.2f}): {len(low_score_wallets)}")
    print(f"Number of high score wallets (score >= {high_score_threshold:.2f}): {len(high_score_wallets)}")


    # Load the full wallet features again to access more metrics
    try:
        wallet_features_df = pd.read_csv('wallet_features.csv')
        # Merge with scores
        comparison_df = pd.merge(scored_wallets_df[['userWallet', 'credit_score', 'liquidation_rate']],
                                 wallet_features_df,
                                 on='userWallet',
                                 how='left')

        comparison_df['score_group'] = 'Mid'
        comparison_df.loc[comparison_df['credit_score'] <= low_score_threshold, 'score_group'] = 'Low'
        comparison_df.loc[comparison_df['credit_score'] >= high_score_threshold, 'score_group'] = 'High'

        # Compare metrics
        comparison_metrics = comparison_df.groupby('score_group')[['num_transactions', 'active_days', 'tx_frequency', 'num_liquidation_calls', 'liquidation_rate']].mean()

        print("\nAverage Metrics by Score Group:")
        print(comparison_metrics)

        # Visualizing comparison (e.g., average liquidations by group)
        plt.figure(figsize=(8, 5))
        sns.barplot(x=comparison_metrics.index, y=comparison_metrics['liquidation_rate'])
        plt.title('Average Liquidation Rate by Wallet Score Group')
        plt.xlabel('Score Group')
        plt.ylabel('Average Liquidation Rate')
        plt.show()

        plt.figure(figsize=(8, 5))
        sns.barplot(x=comparison_metrics.index, y=comparison_metrics['num_transactions'])
        plt.title('Average Number of Transactions by Wallet Score Group')
        plt.xlabel('Score Group')
        plt.ylabel('Average Number of Transactions')
        plt.show()


    except FileNotFoundError:
        print("Warning: 'wallet_features.csv' not found. Skipping detailed comparison.")


    # 3. Highlight potential risk patterns (bots, exploiters, good actors)
    # This requires more sophisticated analysis and domain knowledge,
    # but we can look at extreme values or specific combinations of features.

    print("\nHighlighting Potential Risk Patterns (Based on Scores & Liquidation Rate)...")

    # Example: Wallets with very low scores and high liquidation rates
    potential_risky_wallets = scored_wallets_df[
        (scored_wallets_df['credit_score'] < scored_wallets_df['credit_score'].quantile(0.10)) & # Bottom 10% score
        (scored_wallets_df['liquidation_rate'] > 0.5) # More than 50% of transactions involved liquidation calls
    ]

    print(f"\nPotential risky wallets (Low Score & High Liquidation Rate): {len(potential_risky_wallets)}")
    if not potential_risky_wallets.empty:
        print(potential_risky_wallets.head())

    # Example: Wallets with very high scores (potentially good actors)
    potential_good_actors = scored_wallets_df[
        (scored_wallets_df['credit_score'] > scored_wallets_df['credit_score'].quantile(0.90)) & # Top 10% score
        (scored_wallets_df['liquidation_rate'] == 0) # No liquidations
    ]

    print(f"\nPotential good actors (High Score & No Liquidations): {len(potential_good_actors)}")
    if not potential_good_actors.empty:
        print(potential_good_actors.head())


    # NOTE: Identifying bots or exploiters accurately requires more advanced techniques
    # like anomaly detection, clustering, and potentially external data sources.
    # The examples above are simple indicators based on the calculated score and liquidation rate.

else:
    print("\nSkipping analysis and visualization due to empty data.")


# Create a dummy analysis.md file content
analysis_md_content = """
# Wallet Behavior Analysis

## Goal
Understand wallet behavior patterns across different credit score ranges.

## Methodology
1.  Analyzed the distribution of calculated wallet credit scores.
2.  Compared key behavioral metrics (number of transactions, active days, liquidation rate) for wallets in low, medium, and high score groups.
3.  Highlighted potential risk and good actor patterns based on score and liquidation rate combinations.

## Results

### 1. Credit Score Distribution
The following chart shows the distribution of wallet credit scores across different ranges (0-100, 100-200, ..., 900-1000).

![Score Distribution Chart Placeholder](score_distribution.png)
*Note: The actual chart will be displayed in the notebook output.*

The distribution helps understand the overall spread of scores in the dataset.

### 2. High vs. Low Score Wallet Comparison
We compared wallets categorized into low, medium, and high score groups based on score quantiles (e.g., bottom 25% low, top 25% high). The table below shows the average values for key behavioral metrics for each group.

| Score Group | Average Transactions | Average Active Days | Average Transaction Frequency | Average Liquidation Calls | Average Liquidation Rate |
|-------------|----------------------|---------------------|-------------------------------|---------------------------|--------------------------|
| Low         | [Average from data]  | [Average from data] | [Average from data]           | [Average from data]       | [Average from data]      |
| Mid         | [Average from data]  | [Average from data] | [Average from data]           | [Average from data]       | [Average from data]      |
| High        | [Average from data]  | [Average from data] | [Average from data]           | [Average from data]       | [Average from data]      |

*Note: The actual average values are printed in the notebook output.*

This comparison reveals that wallets with higher scores tend to have (e.g., more transactions, lower liquidation rates), while low-score wallets might exhibit different behaviors.

### 3. Potential Risk and Good Actor Patterns
Based on the scores and calculated liquidation rates, we identified potential groups:

*   **Potential Risky Wallets:** Wallets with very low scores and a high percentage of transactions involving liquidations. These might warrant further investigation.
*   **Potential Good Actors:** Wallets with very high scores and no recorded liquidations. These likely represent stable and low-risk participants.

*Note: Specific wallet addresses and counts for these groups are printed in the notebook output. Identifying bots or sophisticated exploiters requires more advanced detection methods.*

## Conclusion
The analysis provides initial insights into how wallet behavior correlates with the calculated credit score. High-scoring wallets generally show characteristics of more active and less risky participants, while low scores can indicate potentially riskier behavior like higher liquidation rates. Further analysis, including clustering and anomaly detection, could refine the understanding of different wallet archetypes.
"""

# Replace placeholders with actual values if comparison data is available
if 'comparison_metrics' in locals() and not comparison_metrics.empty:
    for group in comparison_metrics.index:
        for metric in comparison_metrics.columns:
            placeholder = f"[Average from data]"
            actual_value = f"{comparison_metrics.loc[group, metric]:.2f}"
            analysis_md_content = analysis_md_content.replace(f"| {group}         | {placeholder}", f"| {group}         | {actual_value}", 1)
            analysis_md_content = analysis_md_content.replace(f"| {placeholder}  | {group}         ", f"| {actual_value}  | {group}         ", 1) # Handle potential misplaced placeholders

# Write the content to analysis.md
with open('analysis.md', 'w') as f:
    f.write(analysis_md_content)

print("\nCreated analysis.md with a summary of the findings.")
print("Please review the notebook output for the actual charts and specific data.")

# @title Default title text
# prompt: Score distribution graph
# Insights on wallet behavior across score bands
# Anomalies and edge cases

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import os

# Load the scored data (should now include liquidation_rate from previous step)
scored_wallets_df_path = 'scored_wallet_features.csv'
wallet_features_df_path = 'wallet_features.csv' # Path to the feature engineered data

data_loaded_successfully = True
try:
    scored_wallets_df = pd.read_csv(scored_wallets_df_path)
    if scored_wallets_df.empty or 'credit_score' not in scored_wallets_df.columns:
        print("Error: Scored data is empty or does not contain 'credit_score' column.")
        data_loaded_successfully = False
    else:
        print("Scored data loaded successfully.")

    # Load the full wallet features again to merge with scores for detailed analysis
    if os.path.exists(wallet_features_df_path):
        wallet_features_df = pd.read_csv(wallet_features_df_path)
        if wallet_features_df.empty:
             print("Warning: Wallet features data is empty.")
             # Create an empty dataframe with expected columns to avoid errors
             wallet_features_df = pd.DataFrame(columns=['userWallet', 'num_deposits', 'num_borrows', 'num_liquidation_calls', 'first_tx', 'last_tx', 'num_transactions', 'active_days', 'tx_frequency'])

        print("Wallet features data loaded successfully.")
    else:
        print(f"Warning: Wallet features file not found at {wallet_features_df_path}. Some analysis details may be missing.")
        # Create an empty dataframe to avoid errors
        wallet_features_df = pd.DataFrame(columns=['userWallet', 'num_deposits', 'num_borrows', 'num_liquidation_calls', 'first_tx', 'last_tx', 'num_transactions', 'active_days', 'tx_frequency'])


except FileNotFoundError:
    print(f"Error: Required data file(s) not found. Please run the previous steps.")
    data_loaded_successfully = False
except Exception as e:
    print(f"An error occurred while loading data: {e}")
    data_loaded_successfully = False


if data_loaded_successfully and not scored_wallets_df.empty:

    # Merge scored data with wallet features for comprehensive analysis
    # Ensure the merge key 'userWallet' exists in both dataframes
    if 'userWallet' in scored_wallets_df.columns and 'userWallet' in wallet_features_df.columns:
         combined_df = pd.merge(scored_wallets_df, wallet_features_df, on='userWallet', how='left', suffixes=('_scored', '_features'))
         # Handle potential duplicate columns after merge if any (e.g., 'num_transactions')
         # Prioritize columns from the 'features' dataframe if available, otherwise use the 'scored' one
         for col in ['num_deposits', 'num_borrows', 'num_liquidation_calls', 'num_transactions', 'active_days', 'tx_frequency']:
             if f'{col}_features' in combined_df.columns:
                  combined_df[col] = combined_df[f'{col}_features']
                  combined_df = combined_df.drop(columns=[f'{col}_features'])
             if f'{col}_scored' in combined_df.columns and col != 'num_transactions': # Keep num_transactions from scored if features missing
                  combined_df = combined_df.drop(columns=[f'{col}_scored'])

         print("Scored data and wallet features merged successfully.")
    elif scored_wallets_df.empty and wallet_features_df.empty:
        print("Both scored data and wallet features dataframes are empty. Skipping analysis.")
        data_loaded_successfully = False # Set to False to skip analysis
    else:
        print("Error: 'userWallet' column missing in one of the dataframes. Cannot merge.")
        data_loaded_successfully = False


if data_loaded_successfully and not combined_df.empty:

    # 1. Score distribution graph
    print("\nGenerating Score Distribution Graph...")
    # Re-calculate score bins and distribution just in case the dataframe changed or was reloaded
    score_bins = range(0, 1001, 100)
    # Ensure 'credit_score' is numeric and handle potential errors
    combined_df['credit_score'] = pd.to_numeric(combined_df['credit_score'], errors='coerce').fillna(0).astype(int)

    score_labels = [f'{i}-{i+99}' for i in score_bins[:-1]]
    score_labels[-1] = '900-1000' # Adjust the last label

    combined_df['score_bin'] = pd.cut(combined_df['credit_score'],
                                      bins=score_bins,
                                      right=False,
                                      labels=score_labels,
                                      include_lowest=True)

    # Count wallets per bin
    score_distribution = combined_df['score_bin'].value_counts().sort_index()

    plt.figure(figsize=(12, 6))
    sns.barplot(x=score_distribution.index, y=score_distribution.values, palette='viridis')
    plt.title('Distribution of Wallet Credit Scores (0-1000)')
    plt.xlabel('Credit Score Range')
    plt.ylabel('Number of Wallets')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.show()

    # 2. Insights on wallet behavior across score bands
    print("\nProviding Insights on Wallet Behavior Across Score Bands...")

    # Define score bands
    score_bands = {
        'Very Low (0-200)': (0, 200),
        'Low (200-400)': (200, 400),
        'Mid (400-600)': (400, 600),
        'High (600-800)': (600, 800),
        'Very High (800-1000)': (800, 1000)
    }

    # Create a column for score bands
    combined_df['score_band'] = 'Other' # Default
    for band, (lower, upper) in score_bands.items():
        combined_df.loc[(combined_df['credit_score'] >= lower) & (combined_df['credit_score'] < upper), 'score_band'] = band

    # Ensure score_band is ordered correctly for plotting
    ordered_bands = list(score_bands.keys())
    combined_df['score_band'] = pd.Categorical(combined_df['score_band'], categories=ordered_bands, ordered=True)


    # Select relevant columns for aggregation, excluding non-numeric or identifier columns
    behavior_metrics_cols = ['num_deposits', 'num_borrows', 'num_liquidation_calls', 'num_transactions', 'active_days', 'tx_frequency', 'liquidation_rate']
    valid_behavior_metrics = [col for col in behavior_metrics_cols if col in combined_df.columns]

    if valid_behavior_metrics:
        mean_behavior_by_band = combined_df.groupby('score_band')[valid_behavior_metrics].mean()

        print("\nAverage Wallet Behavior Metrics by Credit Score Band:")
        print(mean_behavior_by_band)

        # Visualize key metrics across bands
        metrics_to_plot = ['num_transactions', 'liquidation_rate', 'active_days'] # Choose relevant metrics
        for metric in metrics_to_plot:
            if metric in mean_behavior_by_band.columns:
                plt.figure(figsize=(10, 6))
                sns.barplot(x=mean_behavior_by_band.index, y=mean_behavior_by_band[metric])
                plt.title(f'Average {metric.replace("_", " ").title()} by Credit Score Band')
                plt.xlabel('Credit Score Band')
                plt.ylabel(f'Average {metric.replace("_", " ").title()}')
                plt.xticks(rotation=45, ha='right')
                plt.tight_layout()
                plt.show()
            else:
                print(f"Warning: Metric '{metric}' not found in aggregated data.")


    else:
        print("No valid behavior metrics found for aggregation.")


    # 3. Anomalies and edge cases
    print("\nIdentifying Anomalies and Edge Cases...")

    # Anomalies could include:
    # - Wallets with extremely high or low values for certain features (e.g., huge number of txs, 0 active days but > 0 txs).
    # - Wallets with unexpected combinations of features (e.g., high deposits but high liquidations).
    # - Wallets with a credit score of 0 or 1000 (if they are outliers).

    # Example: Wallets with unusually high transaction counts (e.g., > 99th percentile)
    if not combined_df.empty and 'num_transactions' in combined_df.columns:
        tx_threshold = combined_df['num_transactions'].quantile(0.99)
        high_tx_wallets = combined_df[combined_df['num_transactions'] > tx_threshold]
        print(f"\nWallets with unusually high transaction counts (>{tx_threshold:.0f}): {len(high_tx_wallets)}")
        if not high_tx_wallets.empty:
            display(high_tx_wallets[['userWallet', 'credit_score', 'num_transactions', 'tx_frequency', 'liquidation_rate']].head())
    else:
         print("Could not check for high transaction count anomalies (dataframe empty or num_transactions missing).")


    # Example: Wallets with non-zero transactions but 0 active days (could indicate data issue or specific pattern)
    if not combined_df.empty and 'num_transactions' in combined_df.columns and 'active_days' in combined_df.columns:
        zero_active_days_wallets = combined_df[(combined_df['active_days'] == 0) & (combined_df['num_transactions'] > 0)]
        print(f"\nWallets with 0 active days but > 0 transactions: {len(zero_active_days_wallets)}")
        if not zero_active_days_wallets.empty:
             # Load the original transaction data if needed to inspect these wallets' transactions
             # For now, just show their features
             display(zero_active_days_wallets[['userWallet', 'credit_score', 'num_transactions', 'first_tx', 'last_tx']].head())
    else:
         print("Could not check for zero active days anomalies (dataframe empty or relevant columns missing).")


    # Example: Wallets at the extreme ends of the score (0 or 1000) - potentially due to edge case feature values
    if not combined_df.empty and 'credit_score' in combined_df.columns:
        # Filter the combined_df AFTER the merge and column handling
        extreme_score_wallets = combined_df[(combined_df['credit_score'] == 0) | (combined_df['credit_score'] == 1000)].copy()
        print(f"\nWallets with extreme scores (0 or 1000): {len(extreme_score_wallets)}")
        if not extreme_score_wallets.empty:
             print("\nColumns in extreme_score_wallets before display:")
             print(extreme_score_wallets.columns) # Print columns here

             # Now extreme_score_wallets is a filtered view of combined_df
             # Ensure the columns to display exist in extreme_score_wallets
             cols_to_display = ['userWallet', 'credit_score', 'num_transactions', 'num_liquidation_calls', 'liquidation_rate', 'active_days']
             existing_cols_to_display = [col for col in cols_to_display if col in extreme_score_wallets.columns]
             if existing_cols_to_display:
                display(extreme_score_wallets[existing_cols_to_display].head())
             else:
                print("Required columns for displaying extreme score wallets not found.")
        else:
            print("No wallets found with extreme scores (0 or 1000).")
    else:
        print("Could not check for extreme score anomalies (dataframe empty or credit_score missing).")


    # Further anomaly detection would involve:
    # - Visualizing features (scatter plots, box plots) to spot outliers.
    # - Using anomaly detection algorithms (e.g., Isolation Forest, Local Outlier Factor) on the feature space.
    # - Investigating specific transaction patterns for suspicious wallets.

    print("\nBasic anomaly check complete. More sophisticated anomaly detection methods can be applied for deeper insights.")

else:
    print("\nSkipping analysis and visualization due to data loading or merging issues.")